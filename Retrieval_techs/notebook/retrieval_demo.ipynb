{"cells":[{"cell_type":"markdown","metadata":{"id":"oHqyID7jG5SR"},"source":["# Envaironment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urdV_yPFG5S6"},"outputs":[],"source":["# !pip install openai\n","# !pip install langchain\n","# !pip install azure-identity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6APAAn_G5S7"},"outputs":[],"source":["import os\n","\n","openaiAPIVersion = os.getenv(\"OPENAI_API_VERSION\")\n","gpt4Model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_GPT4\")\n","gpt35Model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_GPT35\")\n","embeddingModel = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_EMBEDDING\")\n","\n","from langchain.chat_models import AzureChatOpenAI\n","llm = AzureChatOpenAI(\n","    temperature=0,\n","    deployment_name=gpt35Model,\n","    openai_api_version=openaiAPIVersion,\n",")\n","\n","from langchain.embeddings import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings(deployment=embeddingModel)"]},{"cell_type":"markdown","metadata":{"id":"bs577QgGG5S8"},"source":["# Create vector embeddings db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLdPgNk2G5S9"},"outputs":[],"source":["# ! pip install chromadb"]},{"cell_type":"markdown","metadata":{"id":"XdtFH24NG5S9"},"source":["## small story vector db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8GpYjn1G5S-"},"outputs":[],"source":["# !rm -rf ./db/story"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7PCU6x9G5S-","outputId":"7d013c17-6730-4ebf-979a-9e5e3b1a5e23"},"outputs":[{"name":"stdout","output_type":"stream","text":["9 page_content='In a tranquil marshland, surrounded by tall reeds and the soft glow of fireflies, a particular event was in the making. I remember that night vividly...' metadata={}\n","9\n"]}],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","\n","story_text = \"\"\"\\\n","In a tranquil marshland, surrounded by tall reeds and the soft glow of fireflies, a particular event was in the making. \\\n","I remember that night vividly... toads, along with frogs, were all busy planning a moonlit celebration.\n","\n","The idea had started when some young frogs saw the radiant full moon and said, \"Look at how she shines! We should have a celebration.\" \\\n","And the frogs and the toads said: \"Let us have a party tonight, as the moon is shining.\" \\\n","Not wanting to miss out on a chance for festivities, there was a party under the moon that all toads, with the frogs, decided to throw that night.\n","\n","As the details were hashed out and the excitement grew, the frogs and the toads were meeting in the night for a party under the moon. \\\n","Amidst their discussions, they wanted the event to be a memorable one, and someone had a fantastic idea. \\\n","\"What if we all wear purple hats?\" the idea was greeted with enthusiastic croaks and ribbits. \\\n","For the party, frogs and toads set a rule: everyone was to wear a purple hat. \\\n","So everyone put on a purple hat was invited to the party, purple hats is shining by moonlight.\n","\n","The marsh soon buzzed with activity as toads and frogs scrambled to find or make their own unique purple hats. \\\n","It was a night of unity, fun, and fashion, all under the silvery gaze of the moon.\\\n","\"\"\"\n","\n","small_text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 200,\n","    chunk_overlap  = 0,\n","    separators = [\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \"(?<=\\\" )\", \" \", \"\",],\n","    is_separator_regex=True\n",")\n","\n","story_text_docs = small_text_splitter.create_documents([story_text])\n","print(len(story_text_docs), story_text_docs[0])\n","\n","story_db_directory = 'db/story/'\n","vectordb = Chroma.from_documents(\n","    documents=story_text_docs,\n","    embedding=embeddings,\n","    persist_directory=story_db_directory\n",")\n","print(vectordb._collection.count())\n"]},{"cell_type":"markdown","metadata":{"id":"U9buCuZvG5TA"},"source":["## multil docs vector db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zquZI7l5G5TB"},"outputs":[],"source":["# !rm -rf ./db/ML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7uDBq6LG5TB","outputId":"83b17797-ffb2-49ba-c407-8fbfc52c6b14"},"outputs":[{"name":"stdout","output_type":"stream","text":["152\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","\n","# Load PDF\n","loaders = [\n","    # Duplicate documents on purpose - messy data\n","    PyPDFLoader(\"../docs/ML/MachineLearning-Lecture01.pdf\"),\n","    PyPDFLoader(\"../docs/ML/MachineLearning-Lecture02.pdf\"),\n","    PyPDFLoader(\"../docs/ML/MachineLearning-Lecture03.pdf\"),\n","]\n","ML_docs = []\n","for loader in loaders:\n","    ML_docs.extend(loader.load())\n","\n","# Split\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1500,\n","    chunk_overlap = 150\n",")\n","ML_text_segments = text_splitter.split_documents(ML_docs)\n","print(ML_text_segments[0].page_content[0:200],'\\n', ML_text_segments[0].metadata)\n","\n","# embedding\n","ML_db_directory = 'db/ML/'\n","ML_vectordb = Chroma(\n","    embedding_function=embeddings,\n","    persist_directory=ML_db_directory\n",")\n","for segment in ML_text_segments:\n","    ML_vectordb.add_documents([segment])\n","print(ML_vectordb._collection.count())"]},{"cell_type":"markdown","metadata":{"id":"B3BlQwK-G5TB"},"source":["# MMR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bReQCP8cG5TC"},"outputs":[],"source":["story_question = 'Tell me about the party that night. Using concise tone.'"]},{"cell_type":"markdown","metadata":{"id":"qavbd8b3G5TC"},"source":["### Fail mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpAYNpetG5TC","outputId":"cc63a3aa-5c00-484f-ebc9-0ef59d2a632d"},"outputs":[{"name":"stdout","output_type":"stream","text":["The frogs and toads threw a party under the moon in a tranquil marshland surrounded by tall reeds and fireflies.\n","\n","--------------------------\n","\n","As the details were hashed out and the excitement grew, the frogs and the toads were meeting in the night for a party under the moon.\n","In a tranquil marshland, surrounded by tall reeds and the soft glow of fireflies, a particular event was in the making. I remember that night vividly...\n","Not wanting to miss out on a chance for festivities, there was a party under the moon that all toads, with the frogs, decided to throw that night.\n"]}],"source":["from langchain.chains import RetrievalQA\n","\n","story_question = 'Tell me about the party that night. Using concise tone.'\n","\n","# vectordb.similarity_search(story_question, k=3)\n","\n","story_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 3}),\n",")\n","\n","result = story_qa_chain({\"query\": story_question})\n","\n","print(result[\"result\"])\n","print('\\n--------------------------\\n')\n","for doc in result[\"source_documents\"]:\n","    print(doc.page_content)"]},{"cell_type":"markdown","metadata":{"id":"YIfYpGPMG5TC"},"source":["### With MMR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gEO8iDeG5TD","outputId":"cddbb74b-adf2-4d76-adeb-fae8aa2d0826"},"outputs":[{"name":"stdout","output_type":"stream","text":["The party that night was held in a tranquil marshland, surrounded by tall reeds and the soft glow of fireflies. The frogs and toads gathered under the moon, wearing purple hats as per the party rule.\n","\n","--------------------------\n","\n","As the details were hashed out and the excitement grew, the frogs and the toads were meeting in the night for a party under the moon.\n","In a tranquil marshland, surrounded by tall reeds and the soft glow of fireflies, a particular event was in the making. I remember that night vividly...\n","For the party, frogs and toads set a rule: everyone was to wear a purple hat. So everyone put on a purple hat was invited to the party, purple hats is shining by moonlight.\n"]}],"source":["from langchain.chains import RetrievalQA\n","\n","story_question = 'Tell me about the party that night. Using concise tone.'\n","\n","# vectordb.max_marginal_relevance_search(story_question,k=3, fetch_k=5)\n","\n","story_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=vectordb.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 5}),\n",")\n","\n","result = story_qa_chain({\"query\": story_question})\n","\n","print(result[\"result\"])\n","print('\\n--------------------------\\n')\n","for doc in result[\"source_documents\"]:\n","    print(doc.page_content)"]},{"cell_type":"markdown","metadata":{"id":"RDaE7VkSG5TD"},"source":["# Self Quired"]},{"cell_type":"markdown","metadata":{"id":"j4_pgcbYG5TD"},"source":["### Fail mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKKaLnL0G5TD","outputId":"b4599d06-7ffa-4dba-93ed-f1d22b18ca22"},"outputs":[{"name":"stdout","output_type":"stream","text":["In the first lecture, they discussed supervised learning and how it involves teaching the algorithm the correct answers for a set of examples.\n","\n","--------------------------\n","\n","{'page': 2, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 1, 'source': '../docs/ML//MachineLearning-Lecture02.pdf'}\n","{'page': 14, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n"]}],"source":["from langchain.chains import RetrievalQA\n","\n","ML_question = 'What did they say about machine learning algorithm in the first lecture?\\nAnswer by concise tone in a sentence.'\n","\n","# ML_vectordb.similarity_search(ML_question, k=3)\n","\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=ML_vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 3}),\n",")\n","\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print(ML_result[\"result\"])\n","print('\\n--------------------------\\n')\n","for doc in ML_result[\"source_documents\"]:\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{"id":"Bmi2Q1vfG5TE"},"source":["### With self quired"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x77IKNiJG5TE"},"outputs":[],"source":["#!pip install lark==1.1.7\n","#!pip install lark-parser==0.12.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqK_RYmGG5TE","outputId":"301991fc-0e50-4497-be33-d40d520ae680"},"outputs":[{"name":"stdout","output_type":"stream","text":["query='machine learning algorithm' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='source', value='../docs/ML/MachineLearning-Lecture01.pdf') limit=None\n","--------------------------\n","Machine learning algorithms are viewed as a growing new capability for computers, particularly for tasks that are difficult to program by hand, such as reading handwritten characters or flying a helicopter.\n","--------------------------\n","{'page': 2, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 14, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 3, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n"]}],"source":["from langchain.retrievers.self_query.base import SelfQueryRetriever\n","from langchain.chains.query_constructor.base import AttributeInfo\n","\n","metadata_field_info = [\n","    AttributeInfo(\n","        name=\"source\",\n","        description=\"The lecture the chunk is from, should be one of `../docs/ML/MachineLearning-Lecture01.pdf`, `../docs/ML/MachineLearning-Lecture02.pdf`, or `../docs/ML/MachineLearning-Lecture03.pdf`\",\n","        type=\"string\",\n","    ),\n","    AttributeInfo(\n","        name=\"page\",\n","        description=\"The page from the lecture\",\n","        type=\"integer\",\n","    ),\n","]\n","document_content_description = \"Machine learning lecture notes\"\n","self_quired_retriever = SelfQueryRetriever.from_llm(\n","    llm,\n","    ML_vectordb,\n","    document_content_description,\n","    metadata_field_info,\n","    verbose=True,\n","    use_original_query=True,\n","    search_kwargs={'k': 3}\n",")\n","# rdocs = self_quired_retriever.get_relevant_documents(ML_question)\n","# for d in rdocs:\n","#     print(d.metadata)\n","ML_question = 'What did they say about machine learning algorithm in the first lecture?\\nAnswer by concise tone in a sentence.'\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=self_quired_retriever\n",")\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print('--------------------------')\n","print(ML_result[\"result\"])\n","print('--------------------------')\n","for doc in ML_result[\"source_documents\"]:\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{"id":"OC08LWIQG5Tq"},"source":["# Multi quired"]},{"cell_type":"markdown","metadata":{"id":"3z2v_vjzG5Tq"},"source":["### Fail mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBnxf6qTG5Tq","outputId":"ceffbc94-4f75-43e5-c419-7d8f696a7ae6"},"outputs":[{"name":"stdout","output_type":"stream","text":["The technical and application aspects of machine learning are discussed in the class, with a focus on providing students with the skills to apply learning algorithms to various problems.\n","\n","--------------------------\n","\n","{'page': 2, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 10, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 14, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 0, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n"]}],"source":["from langchain.chains import RetrievalQA\n","\n","ML_question = \"What about the technical and application of machine learning?\\nAnswer by concise tone in a sentence.\"\n","\n","# ML_vectordb.similarity_search(ML_question, k=3)\n","\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=ML_vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 4}),\n",")\n","\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print(ML_result[\"result\"])\n","print('\\n--------------------------\\n')\n","for doc in ML_result[\"source_documents\"]:\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{"id":"SGGx-d67G5Tq"},"source":["### Multi quired"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqyKwzNDG5Tr","outputId":"37dfa806-6692-46c2-9395-82563ad3e585"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['What are the technical aspects of machine learning?', 'What are the applications of machine learning?']\n"]},{"name":"stdout","output_type":"stream","text":["--------------------------\n","Machine learning is a growing capability for computers that allows them to perform tasks that are difficult to program by hand, such as reading handwritten characters or flying a helicopter.\n","--------------------------\n","{'page': 2, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n","{'page': 10, 'source': '../docs/ML/MachineLearning-Lecture01.pdf'}\n"]}],"source":["from langchain.retrievers.multi_query import MultiQueryRetriever\n","from langchain.prompts import PromptTemplate\n","import logging\n","\n","logging.basicConfig()\n","logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n","\n","DEFAULT_QUERY_PROMPT = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=\"\"\"You are an AI language model assistant. Your task is\n","    to generate 2 different versions of the given user\n","    question to retrieve relevant documents from a vector  database.\n","    By generating multiple perspectives on the user question,\n","    your goal is to help the user overcome some of the limitations\n","    of distance-based similarity search. Provide these alternative\n","    questions separated by newlines. Original question: {question}\"\"\",\n",")\n","\n","ML_question = \"What about the technical and application of machine learning?\\nAnswer by concise tone in a sentence.\"\n","\n","retriever_from_llm = MultiQueryRetriever.from_llm(\n","    retriever=ML_vectordb.as_retriever(\n","        search_type=\"similarity\", search_kwargs={\"k\": 4}\n","    ),\n","    llm=llm,\n","    prompt=DEFAULT_QUERY_PROMPT,\n",")\n","# retriever_from_llm.get_relevant_documents(query=ML_question)\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=retriever_from_llm\n",")\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print('--------------------------')\n","print(ML_result[\"result\"])\n","print('--------------------------')\n","for doc in ML_result[\"source_documents\"]:\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{"id":"SZEaQ5jeG5Tr"},"source":["# Contextual compression"]},{"cell_type":"markdown","metadata":{"id":"tjCYZiSLG5Tr"},"source":["### Fail Mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP6T5kxpG5Tr","outputId":"cd6f9345-c752-44b6-a9ee-ae8b82fc8782"},"outputs":[{"ename":"InvalidRequestError","evalue":"This model's maximum context length is 4096 tokens. However, your messages resulted in 4220 tokens. Please reduce the length of the messages.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# ML_vectordb.similarity_search(ML_question, k=15) \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ML_qa_chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     llm,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     retriever\u001b[39m=\u001b[39mML_vectordb\u001b[39m.\u001b[39mas_retriever(search_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m, search_kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m14\u001b[39m}),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ML_result \u001b[39m=\u001b[39m ML_qa_chain({\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: ML_question})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(ML_result[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qi.yu/Code/AIGC/SupportAIAzure/notebooks/retrival.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--------------------------\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:139\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    140\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, question\u001b[39m=\u001b[39;49mquestion, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer, \u001b[39m\"\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m\"\u001b[39m: docs}\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:492\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    488\u001b[0m         _output_key\n\u001b[1;32m    489\u001b[0m     ]\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    493\u001b[0m         _output_key\n\u001b[1;32m    494\u001b[0m     ]\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m     )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:105\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    104\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m--> 105\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m    106\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:255\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    241\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:91\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     88\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     89\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 91\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:101\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    102\u001b[0m     prompts,\n\u001b[1;32m    103\u001b[0m     stop,\n\u001b[1;32m    104\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    105\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    106\u001b[0m )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/base.py:414\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    407\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    408\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    412\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    413\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    308\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 309\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    310\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    314\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    297\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 299\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    300\u001b[0m                 m,\n\u001b[1;32m    301\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    302\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    303\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    304\u001b[0m             )\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    447\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    346\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 278\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n","File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n","File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/langchain/chat_models/openai.py:276\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n","File \u001b[0;32m~/Code/AIGC/SupportAIAzure/.venv/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4096 tokens. However, your messages resulted in 4220 tokens. Please reduce the length of the messages."]}],"source":["from langchain.chains import RetrievalQA\n","\n","ML_question = 'What did they say about machine learning?\\nAnswer by concise tone in a sentence.'\n","# ML_vectordb.similarity_search(ML_question, k=14)\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=ML_vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 14}),\n",")\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print(ML_result[\"result\"])\n","print('--------------------------')\n","for doc in ML_result[\"source_documents\"]:\n","    print(doc.page_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7LYI4QPG5Ts"},"outputs":[],"source":["ML_result[\"source_documents\"] = ML_vectordb.similarity_search(ML_question, k=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy8h0-RuG5Ts","outputId":"ec9e9ccc-6110-4170-aeb5-753ca063924d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0: 1498\n","1: 1480\n","2: 515\n","3: 1417\n","4: 1432\n","5: 1444\n","6: 1453\n","7: 1480\n","8: 1492\n","9: 563\n","10: 1316\n","11: 1483\n","12: 1439\n","13: 619\n"]}],"source":["for index, doc in enumerate(ML_result[\"source_documents\"]):\n","    print(f\"{index}: {len(doc.page_content)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nAvmM5OG5Ts","outputId":"ebe777dd-3167-499a-8d83-bef161848387"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------\n","Machine learning grew out of early work in AI and has become a growing new capability for computers, with many applications that cannot be programmed by hand.\n","--------------------------\n","0. \"let's say a few more words about machine learning. I feel that machine learning grew out of early work in AI, early work in artificial intelligence. And over the last — I wanna say last 15 or last 20 years or so, it's been viewed as a sort of growing new capability for computers. And in particular, it turns out that there are many programs or there are many applications that you can't program by hand.\"\n","1. So many students will try to build a cool machine learning application. That's probably the most common project. Some students will try to improve state-of-the-art machine learning. Some of those projects are also very successful.\n","2. someone using some machine learning algorithm and then explain to me what they've been doing for the last six months\n","3. \"Way back in about 1959, Arthur Samuel defined machine learning informally as the [inaudible] that gives computers the ability to learn without being explicitly programmed.\"\n","4. One thing that's sadly not taught in many courses on machine learning is how to take the tools of machine learning and really, really apply them well. But what I plan to do throughout this entire quarter, not just in the segment of learning theory, but actually as a theme running through everything I do this quarter, will be to try to convey to you the skills to really take the learning algorithm ideas and really to get them to work on a problem.\n","5. Arthur Samuel managed to write a checkers program that could play checkers much better than he personally could, and this is an instance of maybe computers learning to do things that they were not programmed explicitly to do.\n","6. machine learning is a highly interdisciplinary topic in which just the TAs find learning algorithms to problems in computer vision and biology and robots and language. And machine learning is one of those things that has and is having a large impact on many applications. Topping the list was actually machine learning. So I think this is a good time to be learning this stuff and learning algorithms and having a large impact on many segments of science and industry.\n","7. So I just want to start by showing you a fun video. Remember at the last lecture, the initial lecture, I talked about supervised learning. And supervised learning was this machine-learning problem where I said we're going to tell the algorithm what the close right answer is for a number of examples, and then we want the algorithm to replicate more of the same.\n","8. I hope convey some of my own excitement about machine learning to you.\n","9. every time you send US mail, you are using a learning algorithm, perhap s without even being aware of it.\n","10. \"to talk a little bit about hidden Markov models, which is a type of machine learning algorithm for modeling time series\"\n","11. There was a project actually done by our TA Paul on improving computer vision algorithms using machine learning.\n","12. The basic idea behind a reinforcement learning algorithm is this idea of what's called a reward function.\n","13. Machine learning is the most exciting field of all the computer sciences.\n"]}],"source":["from langchain.chains import RetrievalQA\n","from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor\n","\n","ML_question = 'What did they say about machine learning?\\nAnswer by concise tone in a sentence.'\n","# ML_vectordb.similarity_search(ML_question, k=14)\n","compressor = LLMChainExtractor.from_llm(llm)\n","compression_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor,\n","    base_retriever=ML_vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 14})\n",")\n","ML_qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    return_source_documents=True,\n","    retriever=compression_retriever,\n",")\n","ML_result = ML_qa_chain({\"query\": ML_question})\n","\n","print('--------------------------')\n","print(ML_result[\"result\"])\n","print('--------------------------')\n","for index, doc in enumerate(ML_result[\"source_documents\"]):\n","    print(f\"{index}. {doc.page_content}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnJxNzSRG5Tt","outputId":"11960a40-942a-4b76-fd8b-28892299d39b"},"outputs":[{"name":"stdout","output_type":"stream","text":["0: 406\n","1: 230\n","2: 116\n","3: 173\n","4: 450\n","5: 225\n","6: 468\n","7: 362\n","8: 70\n","9: 105\n","10: 121\n","11: 112\n","12: 105\n","13: 73\n"]}],"source":["for index, doc in enumerate(ML_result[\"source_documents\"]):\n","    print(f\"{index}: {len(doc.page_content)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9-VArn6G5Tt"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}