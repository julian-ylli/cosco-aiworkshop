import asyncio
import os
import subprocess
from langchain_core.messages import SystemMessage
# from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langgraph.graph import START, StateGraph, MessagesState
from langgraph.prebuilt import tools_condition, ToolNode
from langchain_mcp_adapters.client import MultiServerMCPClient
# åˆ›å»ºMCPå®¢æˆ·ç«¯
def create_mcp_client():
    """åˆ›å»ºMCPå®¢æˆ·ç«¯"""
    try:
        client = MultiServerMCPClient(
            {
                "read_arch_doc": {
                    "command": "python",
                    # "args": ["mcp_server.py"],
                    "args": ["mcp_from_scratch.py"],
                    "transport": "stdio",
                },
                "microsoft.docs.mcp": {
                    "transport": "streamable_http",
                    "url": "https://learn.microsoft.com/api/mcp",
                },
                "awslabs.aws-documentation-mcp-server": {
                    "command": "uvx",
                    "args": ["awslabs.aws-documentation-mcp-server@latest"],
                    "transport": "streamable_http",
                    "transport": "stdio"
                }
            }
        )
        print("âœ… MCPå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        print(f"âŒ åˆ›å»ºMCPå®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯ç”±äºäº‹ä»¶å¾ªç¯å†²çªå¯¼è‡´çš„ï¼Œä½†ä¸ä¼šå½±å“åŸºæœ¬åŠŸèƒ½")
        return None

client = create_mcp_client()


# å…¨å±€å˜é‡å­˜å‚¨å·¥å…·å’ŒLLM
tools = []
llm_with_tools = None

async def initialize_tools():
    """åˆå§‹åŒ–å·¥å…·å’ŒLLM"""
    global tools, llm_with_tools
    
    # è·å–MCPå·¥å…·
    mcp_tools = await client.get_tools()
    
    # åˆå¹¶å·¥å…·åˆ—è¡¨
    # å¯ä»¥åœ¨è¿™é‡Œæ ¹æ®éœ€è¦ç­›é€‰å·¥å…·
    # ä¾‹å¦‚ï¼Œåªé€‰æ‹© 'read_document_with_mcp' å’Œ 'microsoft_docs_search'
    # selected_tool_names = ["read_document_with_mcp", "microsoft_docs_search"]
    # tools = [tool for tool in mcp_tools if tool.name in selected_tool_names]
    tools = mcp_tools
    
    # å®šä¹‰LLMå¹¶ç»‘å®šå·¥å…·

    llm = ChatOpenAI(
        model="gemini-2.5-flash",
        openai_api_key=os.environ.get("OPENAI_API_KEY"),
        openai_api_base=os.environ.get("OPENAI_API_BASE")
    )
    llm_with_tools = llm.bind_tools(tools)
    
    return tools, llm_with_tools

# åŒæ­¥åˆå§‹åŒ–å‡½æ•°ï¼ˆç”¨äºlanggraph devï¼‰
def initialize_tools_sync():
    """åŒæ­¥ç‰ˆæœ¬çš„åˆå§‹åŒ–å‡½æ•°"""
    global tools, llm_with_tools
    if not tools:
        # try:
        tools, llm_with_tools = asyncio.run(initialize_tools())
        # except Exception as e:
        #     print(f"âŒ åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")
            # ä½¿ç”¨æœ¬åœ°å·¥å…·ä½œä¸ºå¤‡ç”¨
    return tools, llm_with_tools

# Node
def confirm(state: MessagesState):
    # ç¡®ä¿å·¥å…·å·²åˆå§‹åŒ–
    if llm_with_tools is None:
        initialize_tools_sync()
    
    # System message
    sys_msg = SystemMessage(
        content="What ever user asked, you should confirm it by a rhetorical question."
    )

    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Node
def assistant(state: MessagesState):
    # ç¡®ä¿å·¥å…·å·²åˆå§‹åŒ–
    if llm_with_tools is None:
        initialize_tools_sync()
    
    sys_msg = SystemMessage(
        content="""
You will act as a senior [Frontend/Backend] Web Programmer. Should answer the user's question based on the tech document provided.
"""
    )
    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# åˆå§‹åŒ–å·¥å…·
initialize_tools_sync()

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")

graph = builder.compile()

# ä¸ºäº†åœ¨langgraph devä¸­è¿è¡Œï¼Œéœ€è¦å¯¼å‡ºgraph
__all__ = ["graph"]
