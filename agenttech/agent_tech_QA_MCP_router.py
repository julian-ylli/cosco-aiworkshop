import asyncio
import os
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import START, END, StateGraph, MessagesState
from langgraph.prebuilt import tools_condition, ToolNode
from langchain_mcp_adapters.client import MultiServerMCPClient
import requests
# åˆ›å»ºMCPå®¢æˆ·ç«¯
class MessagesStateWithVerbosity(MessagesState):
    verbosity: str
def create_mcp_client():
    """åˆ›å»ºMCPå®¢æˆ·ç«¯"""
    try:
        client = MultiServerMCPClient(
            {
                "read_arch_doc": {
                    "command": "python",
                    "args": ["mcp_server.py"],
                    # "args": ["mcp_from_scratch.py"],
                    "transport": "stdio",
                },
                "microsoft.docs.mcp": {
                    "transport": "streamable_http",
                    "url": "https://learn.microsoft.com/api/mcp",
                },
                "awslabs.aws-documentation-mcp-server": {
                    "command": "uvx",
                    "args": ["awslabs.aws-documentation-mcp-server@latest"],
                    "transport": "streamable_http",
                    "transport": "stdio"
                }
            }
        )
        print("âœ… MCPå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        print(f"âŒ åˆ›å»ºMCPå®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯ç”±äºäº‹ä»¶å¾ªç¯å†²çªå¯¼è‡´çš„ï¼Œä½†ä¸ä¼šå½±å“åŸºæœ¬åŠŸèƒ½")
        return None

client = create_mcp_client()


# å…¨å±€å˜é‡å­˜å‚¨å·¥å…·å’ŒLLM
tools = []
llm_with_tools = None
llm_with_tools_pro = None
async def initialize_tools():
    """åˆå§‹åŒ–å·¥å…·å’ŒLLM"""
    global tools, llm_with_tools
    
    # è·å–MCPå·¥å…·
    mcp_tools = await client.get_tools()
    
    # åˆå¹¶å·¥å…·åˆ—è¡¨
    tools = mcp_tools
    print(f"ğŸ“¦ æ€»å·¥å…·æ•°é‡: {len(tools)}")
    
    # å®šä¹‰LLMå¹¶ç»‘å®šå·¥å…·
    llm = ChatOpenAI(
        model="gemini-2.0-flash",
        openai_api_key=os.environ.get("OPENAI_API_KEY"),
        openai_api_base=os.environ.get("OPENAI_API_BASE")
    )
    llm_with_tools = llm.bind_tools(tools)
    llm_pro = ChatOpenAI(
        model="gemini-2.5-flash",
        openai_api_key=os.environ.get("OPENAI_API_KEY"),
        openai_api_base=os.environ.get("OPENAI_API_BASE")
    )
    llm_with_tools_pro = llm_pro.bind_tools(tools)
    return tools, llm_with_tools, llm_with_tools_pro

# åŒæ­¥åˆå§‹åŒ–å‡½æ•°ï¼ˆç”¨äºlanggraph devï¼‰
def initialize_tools_sync():
    """åŒæ­¥ç‰ˆæœ¬çš„åˆå§‹åŒ–å‡½æ•°"""
    global tools, llm_with_tools, llm_with_tools_pro
    if not tools:
        # try:
        tools, llm_with_tools, llm_with_tools_pro = asyncio.run(initialize_tools())
        # except Exception as e:
        #     print(f"âŒ åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")
            # ä½¿ç”¨æœ¬åœ°å·¥å…·ä½œä¸ºå¤‡ç”¨
    return tools, llm_with_tools, llm_with_tools_pro


# Node
def router(state: MessagesStateWithVerbosity):
    # forward the message to classification model deployed on localhost:8080/invocations
    response = requests.post("http://localhost:8080/invocations", json={"inputs": state["messages"][-1].content})
    return {"verbosity": response.json()["predictions"][0]["label"]}

def assistant(state: MessagesStateWithVerbosity):
    # ç¡®ä¿å·¥å…·å·²åˆå§‹åŒ–
    if llm_with_tools is None:
        initialize_tools_sync()
    
    sys_msg = SystemMessage(
        content="""
You will act as a [Frontend/Backend] Web Programmer. Should answer the user's question based on the tech document provided.
"""
    )
    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

def senior_assistant(state: MessagesStateWithVerbosity):
    # ç¡®ä¿å·¥å…·å·²åˆå§‹åŒ–
    if llm_with_tools_pro is None:
        initialize_tools_sync()
    
    sys_msg = SystemMessage(
        content="""
You will act as a senior [Frontend/Backend] Web Programmer. Should answer the user's question based on the tech document provided.
You are more experienced and can provide more detailed and comprehensive answers.
"""
    )
    return {"messages": [llm_with_tools_pro.invoke([sys_msg] + state["messages"])]}

def route_to_assistant(state: MessagesStateWithVerbosity):
    """æ ¹æ®verbosityè·¯ç”±åˆ°ä¸åŒçš„assistant"""
    verbosity = state.get("verbosity", "low")
    if verbosity == "LABEL_1":
        return "senior_assistant"
    elif verbosity == "LABEL_0":
        return "assistant"
    else:
        raise ValueError(f"Unknown verbosity: {verbosity}")

# åˆå§‹åŒ–å·¥å…·
initialize_tools_sync()

# Graph

builder = StateGraph(MessagesStateWithVerbosity)

# Define nodes: these do the work
builder.add_node("router", router)
builder.add_node("assistant", assistant)
builder.add_node("senior_assistant", senior_assistant)
builder.add_node("tools", ToolNode(tools))
tools_pro = tools.copy()
builder.add_node("tools_pro", ToolNode(tools_pro))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "router")
builder.add_conditional_edges(
    "router",
    route_to_assistant,
    {
        "assistant": "assistant",
        "senior_assistant": "senior_assistant"
    }
)

# ä¸ºassistantæ·»åŠ æ¡ä»¶è¾¹
builder.add_conditional_edges(
    "assistant",
    tools_condition,
)

# ä¸ºsenior_assistantæ·»åŠ æ¡ä»¶è¾¹
builder.add_conditional_edges(
    "senior_assistant",
    tools_condition,
    {"tools": "tools_pro", "__end__": END}
)
builder.add_edge("tools", "assistant")
builder.add_edge("tools_pro", "senior_assistant")

graph = builder.compile()

# ä¸ºäº†åœ¨langgraph devä¸­è¿è¡Œï¼Œéœ€è¦å¯¼å‡ºgraph
__all__ = ["graph"]
