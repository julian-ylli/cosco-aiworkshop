import dotenv
import pandas as pd
import numpy as np
import asyncio
import nest_asyncio
import os
from datetime import datetime
import json
from pathlib import Path


# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from agent_tech_QA_MCP import graph
from langchain_core.messages import HumanMessage, ToolMessage
from ragas import EvaluationDataset
from ragas.metrics import ResponseRelevancy

from ragas.llms import LangchainLLMWrapper
from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI
import uuid

class E2ETestEvaluator:
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        # ä½¿ç”¨ Google Gemini ä½œä¸ºè¯„ä¼° LLMï¼Œç¡®ä¿ API key å·²è®¾ç½®
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            print("âš ï¸  GOOGLE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼ŒRAGAS è¯„ä¼°å¯èƒ½å¤±è´¥")

        self.evaluator_llm = LangchainLLMWrapper(ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", api_key=google_api_key))
        self.embedding_llm = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001",api_key=google_api_key)
        self.metrics = [
              ResponseRelevancy(llm=self.evaluator_llm, embeddings=self.embedding_llm),
          ]
        
    async def load_test_data(self, csv_path="tests/data/agent_tech_QA_MCP_data.csv"):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            synth_data = pd.read_csv(csv_path)
            questions = synth_data["questions"].tolist()
            expected_responses = synth_data["responses"].tolist() if "responses" in synth_data.columns else [None]*len(questions)
            expected_trajectories = synth_data["tool_trajectory"].tolist() if "tool_trajectory" in synth_data.columns else [None]*len(questions)
            return questions, expected_responses, expected_trajectories
        except FileNotFoundError:
            print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return [], []
        except KeyError as e:
            print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘åˆ—: {e}")
            return [], [], []

    async def run_e2e_tests(self, questions, expected_trajectories=None, expected_responses=None):
        """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
        outputs = {
            "questions": questions,
            "expected_responses": expected_responses or [],
            "expected_trajectories": expected_trajectories or [],
            "responses": [],
            "tool_trajectory": [],
            "full_trajectory": [],
            "execution_time": []
        }
        
        print(f"ğŸš€ å¼€å§‹è¿è¡Œ {len(questions)} ä¸ªç«¯åˆ°ç«¯æµ‹è¯•...")
        
        for i, q in enumerate(questions):
            print(f"ğŸ“ å¤„ç†é—®é¢˜ {i+1}/{len(questions)}: {q[:50]}...")
            
            thread_id = str(uuid.uuid4())
            config = {"configurable": {"thread_id": thread_id}}
            input_message = HumanMessage(content=q)
            
            # è®°å½•æ‰§è¡Œæ—¶é—´
            start_time = datetime.now()
            
            try:
                output = await graph.ainvoke({"messages": [input_message]}, config)
                
                # æå–å“åº”å’Œè½¨è¿¹
                final_response = output["messages"][-1].content
                tool_trajectory = "[TOOLSEP]".join([
                    m.name for m in output["messages"] 
                    if isinstance(m, ToolMessage)
                ])
                
                # è®°å½•å®Œæ•´è½¨è¿¹
                full_trajectory = []
                for msg in output["messages"]:
                    if isinstance(msg, ToolMessage):
                        full_trajectory.append(f"TOOL:{msg.name}")
                    else:
                        full_trajectory.append(f"MSG:{type(msg).__name__}")
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                outputs["responses"].append(final_response)
                outputs["tool_trajectory"].append(tool_trajectory)
                outputs["full_trajectory"].append(" -> ".join(full_trajectory))
                outputs["execution_time"].append(execution_time)
                
                print(f"âœ… å®Œæˆ - å“åº”é•¿åº¦: {len(final_response)}, å·¥å…·è°ƒç”¨: {len(tool_trajectory.split('[TOOLSEP]')) if tool_trajectory else 0}")
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                outputs["responses"].append(f"ERROR: {str(e)}")
                outputs["tool_trajectory"].append("")
                outputs["full_trajectory"].append("ERROR")
                outputs["execution_time"].append(0)
        
        return outputs
    
    async def evaluate_with_ragas(self, outputs):
        """ä½¿ç”¨ RAGAS è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼Œç¡®ä¿å¯¹æ¯”æœŸæœ›è¾“å‡ºå’Œå®é™…è¾“å‡º"""
        print("ğŸ” å¼€å§‹ RAGAS è¯„ä¼°...")
        results = {}
        eval_data = []
        expected_responses = outputs.get("expected_responses", [])
        for i, (question, response) in enumerate(zip(outputs["questions"], outputs["responses"])):
            if not response.startswith("ERROR"):
                ground_truth = expected_responses[i] if expected_responses and i < len(expected_responses) else ""
                eval_data.append({
                    "question": str(question),
                    "answer": str(response),
                    "ground_truth": ground_truth
                })
        if not eval_data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ç”¨äºè¯„ä¼°")
            return results
        try:
            from ragas import SingleTurnSample
            samples = []
            for item in eval_data:
                try:
                    sample = SingleTurnSample(
                        user_input=item["question"],
                        response=item["answer"],
                        reference=item["ground_truth"]
                    )
                    samples.append(sample)
                except Exception as e:
                    print(f"âŒ åˆ›å»ºæ ·æœ¬å¤±è´¥: {e}")
                    print(f"   é—®é¢˜: {item['question'][:50]}...")
                    continue
            if not samples:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬ç”¨äºè¯„ä¼°")
                return results
            dataset = EvaluationDataset(samples)
            from ragas import evaluate
            print(f"ğŸ“Š å¼€å§‹è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬...")
            evaluation_results = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                # llm=self.evaluator_llm,
                show_progress=True
            )
            for metric in self.metrics:
                try:
                    score = evaluation_results[metric.name]
                    # å¦‚æœæ˜¯listï¼Œå–å‡å€¼ç”¨äºæ‰“å°å’ŒHTMLå±•ç¤º
                    if isinstance(score, list):
                        score_value = float(np.mean(score)) if len(score) > 0 else 0.0
                        results[metric.name] = score  # ä¿ç•™åŸå§‹listç”¨äºåç»­HTMLæ¯æ¡ç”¨ä¾‹å±•ç¤º
                    else:
                        score_value = float(score)
                        results[metric.name] = score
                    print(f"   {metric.name}: {score_value:.4f}")
                except Exception as e:
                    print(f"âŒ æå– {metric.name} åˆ†æ•°å¤±è´¥: {e}")
                    results[metric.name] = None
        except Exception as e:
            print(f"âŒ RAGAS è¯„ä¼°å¤±è´¥: {e}")
            results["error"] = str(e)
        return results
    
    def analyze_trajectory(self, outputs):
        """åˆ†æå·¥å…·è°ƒç”¨è½¨è¿¹"""
        print("ğŸ›¤ï¸  åˆ†æå·¥å…·è°ƒç”¨è½¨è¿¹...")
        
        trajectory_analysis = {
            "total_questions": len(outputs["questions"]),
            "successful_runs": len([r for r in outputs["responses"] if not r.startswith("ERROR")]),
            "tool_usage_stats": {},
            "trajectory_patterns": {},
            "execution_time_stats": {}
        }
        
        # åˆ†æå·¥å…·ä½¿ç”¨æƒ…å†µ
        all_tools = []
        for trajectory in outputs["tool_trajectory"]:
            if trajectory:
                tools = trajectory.split("[TOOLSEP]")
                all_tools.extend(tools)
        
        from collections import Counter
        tool_counts = Counter(all_tools)
        trajectory_analysis["tool_usage_stats"] = dict(tool_counts)
        
        # åˆ†ææ‰§è¡Œæ—¶é—´
        valid_times = [t for t in outputs["execution_time"] if t > 0]
        if valid_times:
            trajectory_analysis["execution_time_stats"] = {
                "avg_time": sum(valid_times) / len(valid_times),
                "min_time": min(valid_times),
                "max_time": max(valid_times),
                "total_time": sum(valid_times)
            }
        
        # åˆ†æè½¨è¿¹æ¨¡å¼
        trajectory_patterns = Counter(outputs["full_trajectory"])
        trajectory_analysis["trajectory_patterns"] = dict(trajectory_patterns)
        
        return trajectory_analysis
    
    def analyze_trajectory_accuracy(self, outputs):
        """åˆ†æè½¨è¿¹å‡†ç¡®ç‡"""
        print("ğŸ¯ åˆ†æè½¨è¿¹å‡†ç¡®ç‡...")
        
        if not outputs.get("expected_trajectories"):
            print("âš ï¸  æ²¡æœ‰é¢„æœŸè½¨è¿¹æ•°æ®ï¼Œè·³è¿‡è½¨è¿¹å‡†ç¡®ç‡åˆ†æ")
            return {
                "trajectory_accuracy": 0.0,
                "total_comparisons": 0,
                "correct_trajectories": 0,
                "trajectory_details": []
            }
        
        trajectory_accuracy_analysis = {
            "trajectory_accuracy": 0.0,
            "total_comparisons": 0,
            "correct_trajectories": 0,
            "trajectory_details": []
        }
        
        correct_count = 0
        total_count = 0
        
        for i, (expected, actual) in enumerate(zip(
            outputs["expected_trajectories"], 
            outputs["tool_trajectory"]
        )):
            if i >= len(outputs["questions"]):
                break
                
            total_count += 1
            
            # æ ‡å‡†åŒ–è½¨è¿¹æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
            expected_clean = expected.strip() if expected else ""
            actual_clean = actual.strip() if actual else ""
            
            # æ£€æŸ¥è½¨è¿¹æ˜¯å¦åŒ¹é…
            is_correct = expected_clean == actual_clean
            
            if is_correct:
                correct_count += 1
            
            trajectory_detail = {
                "question_index": i,
                "question": outputs["questions"][i][:50] + "..." if len(outputs["questions"][i]) > 50 else outputs["questions"][i],
                "expected_trajectory": expected_clean,
                "actual_trajectory": actual_clean,
                "is_correct": is_correct,
                "response_status": "ERROR" if outputs["responses"][i].startswith("ERROR") else "SUCCESS"
            }
            
            trajectory_accuracy_analysis["trajectory_details"].append(trajectory_detail)
        
        if total_count > 0:
            trajectory_accuracy_analysis["trajectory_accuracy"] = correct_count / total_count
            trajectory_accuracy_analysis["total_comparisons"] = total_count
            trajectory_accuracy_analysis["correct_trajectories"] = correct_count
        
        print(f"ğŸ“Š è½¨è¿¹å‡†ç¡®ç‡: {trajectory_accuracy_analysis['trajectory_accuracy']:.2%} ({correct_count}/{total_count})")
        
        return trajectory_accuracy_analysis
    
    def generate_html_report(self, outputs, ragas_results, trajectory_analysis, trajectory_accuracy_analysis):
        """ç”Ÿæˆ HTML æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆ HTML æŠ¥å‘Š...")
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>E2E æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .metric {{ display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 5px; }}
        .success {{ color: #28a745; }}
        .error {{ color: #dc3545; }}
        .warning {{ color: #ffc107; }}
        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .trajectory {{ font-family: monospace; background: #f8f9fa; padding: 5px; border-radius: 3px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ E2E æµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
        <p><strong>æ€»é—®é¢˜æ•°:</strong> {trajectory_analysis['total_questions']}</p>
        <p><strong>æˆåŠŸè¿è¡Œ:</strong> <span class="success">{trajectory_analysis['successful_runs']}</span></p>
        <p><strong>å¤±è´¥è¿è¡Œ:</strong> <span class="error">{trajectory_analysis['total_questions'] - trajectory_analysis['successful_runs']}</span></p>
        <p><strong>æˆåŠŸç‡:</strong> <span class="success">{(trajectory_analysis['successful_runs'] / trajectory_analysis['total_questions'] * 100):.1f}%</span></p>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ RAGAS è¯„ä¼°ç»“æœ</h2>
        <div>
"""
        
        # æ·»åŠ  RAGAS æŒ‡æ ‡
        # åªå±•ç¤ºå‡å€¼
        for metric_name, score in ragas_results.items():
            if isinstance(score, list):
                mean_score = float(np.mean(score)) if len(score) > 0 else 0.0
                color_class = "success" if mean_score > 0.7 else "warning" if mean_score > 0.5 else "error"
                html_content += f'<div class="metric"><strong>{metric_name}å‡å€¼:</strong> <span class="{color_class}">{mean_score:.4f}</span></div>'
            elif score is not None and isinstance(score, (int, float)):
                color_class = "success" if score > 0.7 else "warning" if score > 0.5 else "error"
                html_content += f'<div class="metric"><strong>{metric_name}:</strong> <span class="{color_class}">{score:.4f}</span></div>'
            elif score is not None:
                html_content += f'<div class="metric"><strong>{metric_name}:</strong> <span class="error">{score}</span></div>'
        
        html_content += """
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ¯ è½¨è¿¹å‡†ç¡®ç‡åˆ†æ</h2>
        <p><strong>æ€»ä½“å‡†ç¡®ç‡:</strong> <span class="success">{trajectory_accuracy:.2%}</span></p>
        <p><strong>æ­£ç¡®è½¨è¿¹æ•°:</strong> {correct_trajectories}/{total_comparisons}</p>
        
        <table>
            <tr><th>é—®é¢˜</th><th>é¢„æœŸè½¨è¿¹</th><th>å®é™…è½¨è¿¹</th><th>çŠ¶æ€</th></tr>
""".format(
            trajectory_accuracy=trajectory_accuracy_analysis["trajectory_accuracy"],
            correct_trajectories=trajectory_accuracy_analysis["correct_trajectories"],
            total_comparisons=trajectory_accuracy_analysis["total_comparisons"]
        )
        
        for detail in trajectory_accuracy_analysis["trajectory_details"]:
            status_class = "success" if detail["is_correct"] else "error"
            status_text = "âœ… æ­£ç¡®" if detail["is_correct"] else "âŒ é”™è¯¯"
            html_content += f"""
            <tr>
                <td>{detail['question']}</td>
                <td><span class="trajectory">{detail['expected_trajectory']}</span></td>
                <td><span class="trajectory">{detail['actual_trajectory']}</span></td>
                <td class="{status_class}">{status_text}</td>
            </tr>
"""
        
        html_content += """
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ›¤ï¸ å·¥å…·ä½¿ç”¨ç»Ÿè®¡</h2>
        <table>
            <tr><th>å·¥å…·åç§°</th><th>ä½¿ç”¨æ¬¡æ•°</th></tr>
"""
        
        for tool, count in trajectory_analysis["tool_usage_stats"].items():
            html_content += f"<tr><td>{tool}</td><td>{count}</td></tr>"
        
        html_content += """
        </table>
    </div>
    
    <div class="section">
        <h2>â±ï¸ æ‰§è¡Œæ—¶é—´ç»Ÿè®¡</h2>
"""
        
        if trajectory_analysis["execution_time_stats"]:
            stats = trajectory_analysis["execution_time_stats"]
            html_content += f"""
        <p><strong>å¹³å‡æ‰§è¡Œæ—¶é—´:</strong> {stats['avg_time']:.2f}ç§’</p>
        <p><strong>æœ€çŸ­æ‰§è¡Œæ—¶é—´:</strong> {stats['min_time']:.2f}ç§’</p>
        <p><strong>æœ€é•¿æ‰§è¡Œæ—¶é—´:</strong> {stats['max_time']:.2f}ç§’</p>
        <p><strong>æ€»æ‰§è¡Œæ—¶é—´:</strong> {stats['total_time']:.2f}ç§’</p>
"""
        
        html_content += """
    </div>
    
    <div class="section">
        <h2>ğŸ“ è¯¦ç»†ç»“æœ</h2>
        <table>
            <tr><th>é—®é¢˜</th><th>å“åº”</th><th>å·¥å…·è½¨è¿¹</th><th>æ‰§è¡Œæ—¶é—´</th><th>ç›¸å…³åº¦</th></tr>
"""
        # å…¼å®¹ä¸åŒ key å‘½åï¼ˆå¦‚ answer_relevancy/response_relevancy/ResponseRelevancyï¼‰ï¼Œå¹¶æ‰“å° debug ä¿¡æ¯
        relevancy_scores = None
        for key in ["answer_relevancy", "response_relevancy", "ResponseRelevancy"]:
            if key in ragas_results:
                relevancy_scores = ragas_results[key]
                print(f"[DEBUG] ç›¸å…³åº¦åˆ†æ•° key: {key}, value: {type(relevancy_scores)} -> {relevancy_scores}")
                break
        if relevancy_scores is None:
            print("[DEBUG] æœªæ‰¾åˆ°ç›¸å…³åº¦åˆ†æ•° keyï¼Œragas_results:", ragas_results)
            relevancy_scores = []
        for i, (question, response, trajectory, exec_time) in enumerate(zip(
            outputs["questions"], 
            outputs["responses"], 
            outputs["tool_trajectory"], 
            outputs["execution_time"]
        )):
            status_class = "success" if not response.startswith("ERROR") else "error"
            # å–ç›¸å…³æ€§åˆ†æ•°
            relevancy_str = "-"
            if isinstance(relevancy_scores, list) and i < len(relevancy_scores):
                relevancy = relevancy_scores[i]
                print(f"[DEBUG] ç¬¬{i}æ¡ç›¸å…³åº¦åŸå§‹å€¼: {repr(relevancy)} ç±»å‹: {type(relevancy)}")
                try:
                    relevancy_float = float(relevancy)
                    relevancy_str = f"{relevancy_float:.4f}"
                except (TypeError, ValueError):
                    relevancy_str = str(relevancy) if relevancy is not None else "-"
            elif isinstance(relevancy_scores, (int, float)):
                try:
                    relevancy_float = float(relevancy_scores)
                    relevancy_str = f"{relevancy_float:.4f}"
                except (TypeError, ValueError):
                    relevancy_str = str(relevancy_scores)
            else:
                print(f"[DEBUG] ç¬¬{i}æ¡ç›¸å…³åº¦æ— æ³•è¯†åˆ«ï¼Œrelevancy_scores: {repr(relevancy_scores)} ç±»å‹: {type(relevancy_scores)}")
            # å·¥å…·è½¨è¿¹å±•ç¤ºå°† [TOOLSEP] æ›¿æ¢ä¸ºç®­å¤´
            trajectory_display = (trajectory or 'æ— å·¥å…·è°ƒç”¨').replace('[TOOLSEP]', ' â†’ ')
            html_content += f"""
            <tr>
                <td>{question[:100]}{'...' if len(question) > 100 else ''}</td>
                <td class="{status_class}">{response[:200]}{'...' if len(response) > 200 else ''}</td>
                <td><span class="trajectory">{trajectory_display}</span></td>
                <td>{exec_time:.2f}s</td>
                <td>{relevancy_str}</td>
            </tr>
"""
        html_content += """
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ”„ è½¨è¿¹æ¨¡å¼åˆ†æ</h2>
        <table>
            <tr><th>è½¨è¿¹æ¨¡å¼</th><th>å‡ºç°æ¬¡æ•°</th></tr>
"""
        
        for pattern, count in trajectory_analysis["trajectory_patterns"].items():
            html_content += f"<tr><td><span class='trajectory'>{pattern}</span></td><td>{count}</td></tr>"
        
        html_content += """
        </table>
    </div>
</body>
</html>
"""
        
        # ä¿å­˜ HTML æŠ¥å‘Š
        report_path = Path("tests/reports")
        report_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_file = report_path / f"e2e_test_report_{timestamp}.html"
        
        with open(html_file, "w", encoding="utf-8") as f:
            f.write(html_content)
        
        print(f"âœ… HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
        return str(html_file)
    
    async def run_complete_evaluation(self, csv_path="tests/data/agent_tech_QA_MCP_data.csv"):
        """è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯è¯„ä¼°"""
        print("ğŸ¯ å¼€å§‹å®Œæ•´ç«¯åˆ°ç«¯è¯„ä¼°...")
        
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        questions, expected_responses, expected_trajectories = await self.load_test_data(csv_path)
        if not questions:
            return
        
        # 2. è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
        outputs = await self.run_e2e_tests(questions, expected_trajectories,expected_responses)
        
        # 3. RAGAS è¯„ä¼°
        ragas_results = await self.evaluate_with_ragas(outputs)
        
        # 4. è½¨è¿¹åˆ†æ
        trajectory_analysis = self.analyze_trajectory(outputs)
        
        # 5. è½¨è¿¹å‡†ç¡®ç‡åˆ†æ
        trajectory_accuracy_analysis = self.analyze_trajectory_accuracy(outputs)
        
        # 6. ç”Ÿæˆ HTML æŠ¥å‘Š
        report_path = self.generate_html_report(outputs, ragas_results, trajectory_analysis, trajectory_accuracy_analysis)
        
        # 7. ä¿å­˜è¯¦ç»†ç»“æœ
        results = {
            "outputs": outputs,
            "ragas_results": ragas_results,
            "trajectory_analysis": trajectory_analysis,
            "trajectory_accuracy_analysis": trajectory_accuracy_analysis,
            "report_path": report_path
        }
        
        # ä¿å­˜ JSON ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = Path("tests/reports") / f"e2e_test_results_{timestamp}.json"
        
        # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_results = {
            "outputs": outputs,
            "ragas_results": ragas_results,
            "trajectory_analysis": trajectory_analysis,
            "trajectory_accuracy_analysis": trajectory_accuracy_analysis,
            "report_path": report_path
        }
        
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_file}")
        print(f"ğŸ“Š è¯„ä¼°å®Œæˆï¼æŸ¥çœ‹æŠ¥å‘Š: {report_path}")
        
        return results

# ä¸»æ‰§è¡Œå‡½æ•°
async def main():
    """ä¸»å‡½æ•°"""
    evaluator = E2ETestEvaluator()
    results = await evaluator.run_complete_evaluation()
    return results

# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if __name__ == "__main__":
    asyncio.run(main())

